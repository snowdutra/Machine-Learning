---
hide:
- toc
---

# 03. Pr√©-processamento Avan√ßado para Avalia√ß√£o Robusta

## üßπ A Funda√ß√£o do Sucesso em ML

O pr√©-processamento n√£o √© apenas "limpeza de dados" - √© a **engenharia de funda√ß√£o** que determina o sucesso ou fracasso de qualquer modelo de Machine Learning. Dados mal processados podem tornar at√© mesmo o algoritmo mais sofisticado in√∫til.

## üéØ Impacto Direto na Avalia√ß√£o

### **Por que o Pr√©-processamento Afeta as M√©tricas?**

```python
# ‚ùå ANTES: Dados sem tratamento
Acur√°cia: 0.62
F1-Score: 0.73
AUC-ROC: 0.65

# ‚úÖ DEPOIS: Dados adequadamente processados
Acur√°cia: 0.87  (+40%)
F1-Score: 0.87  (+19%)
AUC-ROC: 0.92   (+42%)
```

## üîß T√©cnicas Fundamentais

### **1. Limpeza de Dados** üßΩ

| Problema | T√©cnica | Impacto na Avalia√ß√£o |
|----------|---------|---------------------|
| **Valores Ausentes** | Imputa√ß√£o inteligente | Evita vi√©s nas m√©tricas |
| **Duplicatas** | Deduplica√ß√£o | Previne data leakage |
| **Outliers** | Detec√ß√£o IQR/Z-score | Melhora robustez |
| **Inconsist√™ncias** | Padroniza√ß√£o de formato | Reduz ru√≠do |

### **2. Transforma√ß√µes Num√©ricas** üìä

| Transforma√ß√£o | Quando Usar | Algoritmos Beneficiados |
|---------------|-------------|------------------------|
| **Normaliza√ß√£o** (Min-Max) | Features em escalas diferentes | KNN, SVM, Redes Neurais |
| **Padroniza√ß√£o** (Z-score) | Distribui√ß√µes normais | PCA, Logistic Regression |
| **Robust Scaling** | Presen√ßa de outliers | Dados financeiros, sensores |
| **Power Transform** | Distribui√ß√µes assim√©tricas | Yeo-Johnson, Box-Cox |

### **3. Encoding de Vari√°veis Categ√≥ricas** üè∑Ô∏è

| T√©cnica | Cardinalidade | Vantagem | Desvantagem |
|---------|---------------|----------|-------------|
| **One-Hot** | Baixa (<10) | Interpret√°vel | Curse of dimensionality |
| **Label Encoding** | Qualquer | Eficiente | Implica ordem |
| **Target Encoding** | Alta | Captura rela√ß√£o com target | Risco de overfitting |
| **Binary Encoding** | Muito alta | Reduz dimens√µes | Menos interpret√°vel |

## üé≠ Tratamento de Desbalanceamento

### **T√©cnicas de Balanceamento** ‚öñÔ∏è

| Abordagem | T√©cnica | Quando Usar | Pr√≥s | Contras |
|-----------|---------|-------------|------|---------|
| **Undersampling** | Random/Tomek Links | Dados abundantes | R√°pido | Perda de informa√ß√£o |
| **Oversampling** | SMOTE/ADASYN | Dados limitados | Preserva informa√ß√£o | Risco de overfitting |
| **H√≠brido** | SMOTEENN | Balanceado | Melhor qualidade | Mais complexo |
| **Algorithmic** | Class weights | Qualquer cen√°rio | Simples | Dependente do algoritmo |

### **Estrat√©gias por N√≠vel de Desbalanceamento** üìä

```python
# Leve (1:2 a 1:4)
‚úÖ Stratified sampling
‚úÖ Class weights

# Moderado (1:5 a 1:20)  
‚úÖ SMOTE + Undersampling
‚úÖ Ensemble methods

# Severo (1:100+)
‚úÖ Anomaly detection
‚úÖ One-class classification
‚úÖ Cost-sensitive learning
```

## üìà Feature Engineering para Melhor Avalia√ß√£o

### **Cria√ß√£o de Features** üî®

| Tipo | Exemplo | Benef√≠cio |
|------|---------|-----------|
| **Polinomiais** | $x_1 \times x_2$, $x_1^2$ | Captura intera√ß√µes |
| **Temporais** | dia_semana, trimestre | Padr√µes sazonais |
| **Agrega√ß√µes** | m√©dia_por_grupo | Contexto estat√≠stico |
| **Ratio Features** | $\frac{vendas}{estoque}$ | Normaliza√ß√£o natural |

### **Sele√ß√£o de Features** üéØ

| M√©todo | Tipo | Vantagem | Limita√ß√£o |
|--------|------|----------|-----------|
| **Filter** | Correla√ß√£o, Chi¬≤ | R√°pido | Ignora intera√ß√µes |
| **Wrapper** | RFE, Forward Selection | Considera modelo | Computacionalmente caro |
| **Embedded** | Lasso, Random Forest | Balanceado | Espec√≠fico do algoritmo |

## üîÑ Pipeline de Pr√©-processamento

### **Sequ√™ncia Recomendada** üìã

```python
1. üîç An√°lise Explorat√≥ria
   ‚îú‚îÄ‚îÄ Distribui√ß√µes
   ‚îú‚îÄ‚îÄ Missing values
   ‚îú‚îÄ‚îÄ Outliers
   ‚îî‚îÄ‚îÄ Correla√ß√µes

2. üßπ Limpeza B√°sica
   ‚îú‚îÄ‚îÄ Remo√ß√£o duplicatas
   ‚îú‚îÄ‚îÄ Tratamento missing
   ‚îî‚îÄ‚îÄ Corre√ß√£o tipos

3. üîß Transforma√ß√µes
   ‚îú‚îÄ‚îÄ Encoding categ√≥ricas
   ‚îú‚îÄ‚îÄ Scaling num√©rico
   ‚îî‚îÄ‚îÄ Feature engineering

4. ‚öñÔ∏è Balanceamento
   ‚îú‚îÄ‚îÄ An√°lise desbalanceamento
   ‚îú‚îÄ‚îÄ Aplica√ß√£o t√©cnica
   ‚îî‚îÄ‚îÄ Valida√ß√£o resultado

5. ‚úÖ Valida√ß√£o Pipeline
   ‚îú‚îÄ‚îÄ Train/validation/test split
   ‚îú‚îÄ‚îÄ Cross-validation
   ‚îî‚îÄ‚îÄ Detec√ß√£o data leakage
```

### **Data Leakage: O Inimigo Silencioso** üö®

| Tipo | Exemplo | Como Detectar | Como Prevenir |
|------|---------|---------------|---------------|
| **Temporal** | Usar dados futuros | Performance irrealista | Split temporal |
| **Target** | Feature correlaciona perfeitamente | Correla√ß√£o = 1.0 | An√°lise de correla√ß√£o |
| **Duplica√ß√£o** | Mesmo registro em train/test | Linhas id√™nticas | Deduplica√ß√£o antes split |

## üéØ Valida√ß√£o do Pr√©-processamento

### **Checklist de Qualidade** ‚úÖ

| Verifica√ß√£o | Crit√©rio | Ferramenta |
|-------------|----------|------------|
| **Distribui√ß√µes** | Similaridade train/test | KS-test, histogramas |
| **Missing Values** | < 5% ou imputado | `df.isnull().sum()` |
| **Outliers** | Identificados e tratados | IQR, Z-score |
| **Scaling** | M√©dia ‚âà 0, Std ‚âà 1 | `df.describe()` |
| **Encoding** | Sem vazamento de info | Cross-validation |

### **M√©tricas de Qualidade dos Dados** üìä

```python
# Completude
completeness = 1 - (missing_values / total_values)

# Consist√™ncia  
consistency = valid_records / total_records

# Acur√°cia dos dados
data_accuracy = correct_values / total_values

# Unicidade
uniqueness = unique_records / total_records
```

## üöÄ Automatiza√ß√£o e Reprodutibilidade

### **Pipeline Automatizado** ü§ñ

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Pipeline reproduz√≠vel
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(drop='first'), categorical_features)
])

# Pipeline completo
ml_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('balancer', SMOTE(random_state=42)),
    ('classifier', KNeighborsClassifier())
])
```

### **Versionamento de Transforma√ß√µes** üìù

| Aspecto | Ferramenta | Benef√≠cio |
|---------|------------|-----------|
| **C√≥digo** | Git | Rastreabilidade |
| **Dados** | DVC | Reprodutibilidade |
| **Modelos** | MLflow | Comparabilidade |
| **Pipelines** | Kedro | Modularidade |

## üîó Integra√ß√£o com Projetos Espec√≠ficos

### **Refer√™ncias Pr√°ticas** üìö

**Pr√©-processamento Aplicado:**

- üå≥ [**√Årvore de Decis√£o**](https://snowdutra.github.io/Machine-Learning/arvore_decisao/08.preprocessamento): 
  - Tratamento de missing values
  - Feature engineering para splits
  - Balanceamento de classes

- üéØ [**KNN**](https://snowdutra.github.io/Machine-Learning/knn/08.preprocessamento):
  - Normaliza√ß√£o cr√≠tica para dist√¢ncias
  - Redu√ß√£o de dimensionalidade
  - Tratamento de outliers

- üé≠ [**K-Means**](https://snowdutra.github.io/Machine-Learning/kmeans/08.preprocessamento):
  - Padroniza√ß√£o obrigat√≥ria
  - Sele√ß√£o de features relevantes
  - Tratamento de ru√≠do

## üí° Dicas Avan√ßadas

### **Para Classifica√ß√£o** üéØ
- Use **stratified sampling** para manter propor√ß√µes
- Aplique **SMOTE** apenas no conjunto de treino
- Valide **calibra√ß√£o** das probabilidades

### **Para Clustering** üé≠
- **Padronize sempre** - dist√¢ncias s√£o sens√≠veis √† escala
- Considere **PCA** para redu√ß√£o de dimensionalidade
- Teste **diferentes m√©tricas** de dist√¢ncia

### **Para S√©ries Temporais** ‚è∞
- **Split temporal** obrigat√≥rio
- **Feature engineering** baseado em lags
- **Stationarity** tests antes do modeling

> **"Dados ruins produzem modelos ruins, independentemente do algoritmo. Dados bem processados podem tornar at√© algoritmos simples efetivos."**

---

*Um pr√©-processamento meticuloso √© o que separa projetos acad√™micos de solu√ß√µes profissionais de Machine Learning.*